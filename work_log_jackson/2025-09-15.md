## Log
- Looked through Glados documentation
	- I think it's usefulness is going to be mainly dependent on how we want to make the system run in parallel, we probably should have a discussion on Thursday about it. If we want a architecture where fpga is running its own experiment, Glados seems to be a good way to go about it. Once the software for running single experiments is finished, all we would really need to do is switch from the configuration file to the Glados parameters. The UI would definitely allow us to run many more experiments than before. However, I think using Glados might introduce some significant restraints on the type of experiments that we can run. Since each compute node would likely end up being a fpga (or small cluster), we would limit our processing power towards evolving a single circuit to that, as the compute nodes can't communicate their results among themselves. I don't think this would really result in any big breakthroughs. Glados would let us run many times more experiments than we currently are, but it wouldn't really allow us to put more processing power towards a single circuit than we are already. 
	  - I'm convinced that the most flexible way making things parallel is to have a single mediator server that takes requests for circuits to be evaluated and forwards them to to fpgas, sending back the results. Than, the main evolution software would just sent requests to this server when a circuits fitness needs to be evaluated. I guess using Glados on the main evolution software instead might work well, as the evaluation mediator would fix the issues mentioned previously. The only concern I have is that the experiments will be fighting over circuit evaluations without any streamlined way to control which gets priority, but this shouldn't be an issue unless we have a lot of Glados nodes. 
	  - This will also allow us to use the already deployed version of Glados since we don't need to be physically connected to the fpga
	  - We *probably* could use kubernetes for the fpga evaluation service, using our own would give us more control over how circuits are evaluated (for example, many experiments would want to always use the same fpga). I tried looking for a framework that allows fine control over task scheduling, but I haven't been able to find anything. 
  - I started to configure bitstreamevolution to run on Glados, realized running the main software on Glados would also mean that we need to reinstall icepacktools for each experiment ran. 
  - Reviewed what I started on last year for distributed computing. 
## Next
- 


[[work_log_jackson/2025-09-12|prev]] [[work_log_jackson/2025-09-16|next]]
