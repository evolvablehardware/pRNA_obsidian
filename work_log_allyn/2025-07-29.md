## Log
### RP + FPGA UART Debugging
Continued to debug my small demo, which uses the uart modules from [this repo](https://github.com/ben-marshall/uart) to echo serial communication. I have a similar demo that works where the modules are not used and instead we `assign UART_TX = UART_RX`. My debugging process:
- Checked that the clock speed (48 HZ) and baud rate (115200) are correct in all places.
	- In the top level module of the verilog, there's params for those values. They are also needed in the C code for the `uart_init` and `ice_fpga_init` function calls. Finally, the pyserial device (or whatever terminal emulator one is using) needs the baud rate. 
- Confirmed that C code for the demo w/ modules and simple demo are the same, minus where I trigger the rest of the modules's state machines.
- Confirmed that the reset signal for the modules was being sent properly
	- Tied the signal to an LED. Note: the LEDs on the 5k device are active low
- Confirmed the pyserial device was being constructed and used in the same way for both demos
- Confirmed the icestorm commands were being called the same way
- Confirmed the demo worked on the 1k device
	- I had to change the clock speed be 12 MHz, some of the icestorm command args, and have the reset signal be triggered using pyserial, but everything else stayed the same. The only thing I changed in the verilog is the clock speed param.

So after all that, I feel confident that everything on the host PC and RP C code matches for the working simple demo and the non-working demo with modules. The verilog code works on the 1k device, so I'm not sure why it wouldn't work on the 5k device. It was originally designed for an Xilinix device, so we would ave run into issues with getting it on the 1k device if there was anything device specific. 

> [!NOTE]- Pyserial code
> ```
> from serial import Serial
> 
> s = Serial("/dev/ttyACM0", 115200, timeout=2)
> while True:
> 	to_send = input("Character to send: ")
> 	byte_to_send = bytes(to_send, "utf-8")
> 	s.write(byte_to_send)
> 	print(f"Sent: {byte_to_send}")
> 	print(f"Received: {s.readline()}")
> ```

> [!NOTE]- Commands to build bitstream
> ```bash
> yosys -p 'synth_ice40 -abc9 -top impl_top -json uart.json' uart_rx.v uart_tx.v impl_top.v
> 
> nextpnr-ice40 --package sg48 --up5k --freq 48 --top impl_top --pcf pins.pcf --json uart.json --asc uart.asc
> 
> icepack uart.asc controller.bin
> 
> cp controller.bin ../../BitstreamEvolutionPico/exampleProjectsC/rp2_ice_bram_controller/controller.bin
> ```

One thing I noticed while debugging is that the picoice shows up under both /dev/ttyACM0 and /dev/ttyACM1, but only the ACM1 can be used for UART communication.
#### Solution 
Brooklyn discovered that some of my pins in the pcf file were messed up, so my UART lines ended up not being connected to anything. Correct pcf file to reference: https://github.com/tinyvision-ai-inc/pico-ice-sdk/blob/main/rtl/pico_ice.pcf.
#### Testbench Issues
I was able to get the full BRAM demo working after fixing that, at least with my shell script. However, when I ran the testbench, everything was taking a really long time failing. I haven't been able to determine what the cause is yet. I think it might have something to with too large of reads/writes being a problem, because when I limited the sized I was testing, everything worked fine. Maybe a buffer in the PI is overflowing or something?
### Further Compression Research
While Brooklyn was helping me debug the UART demo, I read through [Low-complexity and resource-aware compression algorithm for FPGA bitstreams](https://www.researchgate.net/publication/311068056_Low-Complexity_and_Resource-Aware_Compression_Algorithm_for_FPGA_Bitstreams). It describes a compression algorithm to be used with partial bitstreams for FPGAs based on RLE and a bitmask. While the algorithm doesn't have as nice of a compression ration compared to some other algorithms, it has a simple hardware implementation and is fast. [Real-time Bitstream Decompression Scheme for FPGAs Reconfiguration](https://ieeexplore.ieee.org/document/8624003) discusses their hardware implementation, but we will be implementing compression algorithms in C to run on the pi. 

They transform each bitstream into a compressed bitstream and an event list. The event list says when a word in the compressed bitstream should be repeated (RLE), or when the compressed bitstream needs to be XOR'd with a word (BitMask). 

One way I could think to improve this is is to store the comparison bitstream for BitMask on the device, so that the word to XOR with does not need to be included in the event list, just where XOR is needed. I also think it makes sense to first create the BitMask and then do RLE, so we create more consecutive blocks of 0s where two bitstreams are the same. 

I'm not entirely sure what circuit we should compare to to create our bitmask. We could do the circuit currently loaded onto the FPGA, but that might be hard to keep track of. Brooklyn suggested using the blinky circuit, and I think it makes sense to use a single circuit to compare to throughout evolution. It probably would make sense to use a circuit with the same IO as our evolved circuits, so there's more similarities. Maybe we use the first circuit we eval or something?

Another idea could be to figure our what bits we can modify with our mutations, and just send that data. It would be really beneficial for the group at Rose, but not really for anyone not working at a well defined bitstream level.
#### TLDR
- I found a paper that discusses a compression algorithm for FPGA bitstreams using RLE and a bitmask
- I think we can simplify it by using a constant circuit to XOR with to generate our bitmask, but I'm not sure what circuit to use
- We could just send the bits that we are mutating, but this isn't as general of a solution.
## Next
- BRAM/SPRAM Demo
	- Determine why the testbench fails with larger sizes
	- Modify project so that we can compile it for both the 1k and 5k device easily
		- It probably also makes sense to create a separate git repo for the project at this stage, since it's grown so large and I'll need to set up git submodules for the pico-ice tools
	- Add in SPRAM
- Compression
	- Incorporate a bitmask into the RLE algorithm and decide what circuit to XOR with
	- Ask Brooklyn what the data transfer speeds are for bitstreams so I can start to estimate the total amount of time it will take to send and decompress a bitstream

[[2025-07-28|prev]] [[2025-07-30|next]]
