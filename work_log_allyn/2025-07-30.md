## Log
### BRAM Project Refactoring
I decided to create a separate git repo for the BRAM demo, as it has gotten quite large, especially with the need to support multiple devices and implicit/explicit BRAM: https://github.com/rhit-loydma/ice40_memory_controller

I added an option to the makefile to synthesize either implicitly or explicitly instantiated BRAM. I created explicit_bram.v and implicit_bram.v that both contain a bram module with the same inputs an outputs. The makefile controls which verilog file is passed into yosys and how the hex data files are generated (one big file vs one file for each block).

I added an option to the makefile to control which device we compile for. The makefile then passes in the correct number of blocks, clock speed, pcf file, etc. into the verilog files, nextpnr, and helper scripts. Note: for the up5k device: it only generates the bin, not the uf2 file.

Now, the way to build the project looks like:
```bash
make BRAM=[implicit/explicit] DEVICE=[hx1k/up5k]
```

LUT Usage:

|          | implicit | explicit | total LUTS |
| -------- | -------- | -------- | ---------- |
| **hx1k** | 236      | 444      | 1280       |
| **up5k** | 450      | 627      | 5280       |
I started working on integrating pico-ice-sdk into the project so that I can generate uf2 files within the repo, but did not have time to finish.
### Testbench
I updated the testbench code to use the same config file as shell.py

> [!Example Config]-
> ```ini
> [DEVICE]
> # i.e. /dev/ttyUSB0 or /dev/ACM1
> fpga_port = /dev/ttyACM1
> # hx1k or up5k
> device_type = up5k
> 
> [TESTBENCH]
> # random seed for shuffling the tests to run
> random_seed = 0
> # the number of tests to run
> num_tests = 1000
> ```

I also spent some time figuring out what the max size operation we could do with overflowing the RP2040's buffer. I determined we can't go above 27 locations/54 bytes without losing accuracy.
#### Experiment Setup
For each of the different types of BRAM and the 2 devices, I ran 5000 tests with a random seed of 0.

To be able to fairly compare the HX1K and UP5K devices, I tested both devices only on a maximum of 27 locations/54 bytes per command. I also did runs of the hx1k device with a maximum of 256 locations/512 bytes.
#### Results
The collected data can be found [here](https://drive.google.com/drive/folders/1KrpNEEH5dl0t2VeEEP7AvrUvlGK7blNJ?usp=sharing). I also put all of the individual graphs for each run in this [slide deck](https://docs.google.com/presentation/d/1qjKMbTAEZC3W7KUXuwQs4qMoKYVPyxDFNmyEnqyBTjI/edit?usp=sharing).

![[Pasted image 20250730170919.png]]
#### Main Takeaways
- The UP5K device is much faster than the HX1K device, especially when it comes to reading
- If CRAM flashing takes ~25ms on the UP5K devices, then we can do an average of 16 writes (with an average size of 28 bytes, total of 448) before it just makes sense to reflash the device
- There is no significant performance difference between explicit and implicit BRAM
- The explicit BRAM UP5K device does not experience the same issue as the HX1K device, where the first address in every block is read wrong until it is written to
## Next
- BRAM/SPRAM Demo
	- Finish integrating pico-ice-sdk and code to generate uf2 file into the repo, so that it is easier to program the 5k device
	- Write README for the new repo
	- Implement SPRAM operations
- Compression
	- Incorporate a bitmask into the RLE algorithm and decide what circuit to XOR with
	- Ask Brooklyn what the data transfer speeds are for bitstreams so I can start to estimate the total amount of time it will take to send and decompress a bitstream

[[2025-07-29|prev]] [[2025-07-31|next]]
