## Log
### Documentation
I added in documentation for the three verilog files I wrote: bram_group.v, controller.v, and integrated_memory_controller.c:
- bram_group instantiates the individual BRAM blocks and handles reading/writing with a select wire to indicate the block and other typical BRAM control signals. 
- controller.v is a FSM connecting the UART signals to the memory signals. The state machine can be found [[2025-06-27#Updated State Diagram|here]].
- integrated_memory_controller.v is the top level module connecting all of the modules together.

The five other files were not documented as heavily as they were either not written be me or not a core component of the controller:
- pll.v was generated by icepll to allow the clock frequency to change
- tb_uart_bram.v is a simple testbench of the controller
- bram.v instantiates an EBR [(source)](https://github.com/damdoy/ice40_ultraplus_examples/blob/master/bram/implicit_bram.v)
- receiver.v is the UART receiver [(source)](https://github.com/ben-marshall/uart/blob/master/rtl/uart_rx.v)
- transmitter.v is the UART transmitter [(source)](https://github.com/ben-marshall/uart/blob/master/rtl/uart_rx.v)

I also added a short README to the git to explain the project stored in each subdirectory. I didn't go into too much detail, as most were just leading the integrated_memory_controller project. 

### Testbench Visualization
Added in dataviz for the testbench data (see src_python/testbench_viz.ipynb). I plot the mean and std 

The time it takes to read has an interesting relations with the number of locations read. I'm guessing this might have something to do with some buffer within pyserial. The time it takes to write increase linearly.
![[Pasted image 20250630151752.png]]

The is no relationship between the block used and time taken to complete the operation (as expected). 
![[Pasted image 20250630152900.png]]

I also expected the address to have no impact on time, but the address constrains what sizes we can use. For example, we can only read/write 1 location at address 255 of a block, while at address 0 we can read or write 256 locations. So, I think the relationship shown below is an effect of the size relationship.
![[Pasted image 20250630153235.png]]

I plotted the average time (over both reads and writes) as a function of size and address to confirm this hypothesis:
![[Pasted image 20250630162153.png]]

I also plotted the index (order tests were executed in) vs the time it takes to complete the operation. It was mostly consistent throughout. 
![[Pasted image 20250630153643.png]]

I plotted the same variables with the accuracy, but since the accuracy is 100%, there is not anything interesting to show. 

I then figured our we can use pyserial's flush() method to make sure we wait until we have finished transmitting bytes to move on to the next instruction. This gives a more realistic picture of how long writing takes.
![[Pasted image 20250630154324.png]]
![[Pasted image 20250630154412.png]]
![[Pasted image 20250630154331.png]]
![[Pasted image 20250630154343.png]]

### SPI Flash Programming
Started looking at verilog model of SPI flash programmers to find one we could possibly adapt
- https://github.com/sergachev/spi_mem_programmer
- https://github.com/hgeisse/flashprog-DE2-115
- https://zipcpu.com/blog/2018/08/16/spiflash.html
## Next
- Add in more documentation
- Combine the memory controller with a different circuit using warmbooting
- Figure out how to use the FPGA to modify the SPI flash in cases where we're not just modifying the BRAM
- Further iceprog optimizations
	- Being able to specify a list of bitstreams to upload, with a hold time for evaluation, so that we only have to init the FTDI once
- Figure out what is causing the step function in size vs read time

[[2025-06-27|prev]] [[2025-07-01|next]]
