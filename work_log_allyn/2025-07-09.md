## Log
### Explicit vs Implicit BRAM
Looked into what might be causing the differences in behavior when instantiating the two different types of BRAM. I tried a lower clock frequency, which had no effect. I made sure I was explicitly specifying the BRAM is 256x16 based on this [reddit post](https://www.reddit.com/r/yosys/comments/fyxubo/synthesizing_ice40_512x8_block_ram/), but that did not help.  Even after I did that, it still expected the address ports to have a width of 11, so its possible that this might be part of the problem. 

I looked back at the [example](https://github.com/damdoy/ice40_ultraplus_examples/blob/master/bram/explicit_bram.v) I found on github on explicit and implicit BRAM, and they mentioned in a comment of their top level module that the output of the explicit module is delayed by a cycle. I think their issue is that they are wrapping the `SB_RAM40_4K` inst in a module that synchronizes the inputs with a clock, but from my understanding, this is done automatically within the `SB_RAM40_4K` module. Their design also uses too many logic cells to be placed on the 1k chips.

Generally, it seems like it's suggested to use implicit memory. 
### Figuring out where implicit blocks are placed
I started messing around with nextpnr to figure out how to determine which tiles the implicit BRAM blocks are placed on. I figured out we can do this by adding the argument `--debug-placer`. To save the output of nextpnr to file, we can use the argument `-l log.txt`. In the log file, we'll see lines like this:
>Info: AP soln: bram_inst.ram256x16_inst_1_RAM -> X3/Y3/ram

Other cool nextpnr options that might be useful at some point (probably more for the bit stream evolution project):
- `--placed-svg [file].svg` shows which bram blocks/luts/io pins are being used (shown below)
- `--routed-svg [file].svg` shows which spans/nets are in use
![[Pasted image 20250709121144.png|400]] ![[Pasted image 20250709121216.png|400]]

Nextpnr also allows for the ability to run a python script using their api to be run after routing has completed ([example](https://github.com/YosysHQ/nextpnr/blob/xc7/python/dump_design.py)), so this could be an easier way of parsing where the bram tiles are placed than going through the log file. 

There isn't much documentation on how to create those python scripts, but I was eventually able to write `src_python/get_bram_locations.py`:
```python
for cell, cinfo in sorted(ctx.cells, key=lambda x: x.first):
	if(cinfo.type == "ICESTORM_RAM"):
		print("{} : {}".format(cell, cinfo.bel))
```
If we add the argument `--post-route src_python/get_bram_locations.py > temp/top_bram_placement.txt` to our nextpnr-ice40 call, we get:
```text
bram_inst.genblk1[0].implicit_bram_inst.memory.0.0_RAM : X3/Y5/ram
bram_inst.genblk1[10].implicit_bram_inst.memory.0.0_RAM : X10/Y11/ram
bram_inst.genblk1[11].implicit_bram_inst.memory.0.0_RAM : X3/Y11/ram
bram_inst.genblk1[12].implicit_bram_inst.memory.0.0_RAM : X10/Y7/ram
bram_inst.genblk1[13].implicit_bram_inst.memory.0.0_RAM : X3/Y15/ram
bram_inst.genblk1[14].implicit_bram_inst.memory.0.0_RAM : X10/Y1/ram
bram_inst.genblk1[15].implicit_bram_inst.memory.0.0_RAM : X10/Y3/ram
bram_inst.genblk1[1].implicit_bram_inst.memory.0.0_RAM : X3/Y9/ram
bram_inst.genblk1[2].implicit_bram_inst.memory.0.0_RAM : X10/Y5/ram
bram_inst.genblk1[3].implicit_bram_inst.memory.0.0_RAM : X10/Y15/ram
bram_inst.genblk1[4].implicit_bram_inst.memory.0.0_RAM : X3/Y1/ram
bram_inst.genblk1[5].implicit_bram_inst.memory.0.0_RAM : X3/Y3/ram
bram_inst.genblk1[6].implicit_bram_inst.memory.0.0_RAM : X3/Y7/ram
bram_inst.genblk1[7].implicit_bram_inst.memory.0.0_RAM : X10/Y13/ram
bram_inst.genblk1[8].implicit_bram_inst.memory.0.0_RAM : X3/Y13/ram
bram_inst.genblk1[9].implicit_bram_inst.memory.0.0_RAM : X10/Y9/ram
```

I started work on a python script to parse both files and create a dictionary representing the mapping between them. I have not integrated it with the rest of the python code yet.
### Other Progress
#### Simplified memory instantiation
I figured out that we can changed our memory block to have any depth, and yosys will automatically interpret it as multiple blocks. So, I was able to get rid of bram_group.v, and simplify our design to just rely upon bram.v and a single data file.

New start of bram.v:
```verilog
module bram(input wire clk,
input wire rd_en,
input wire wr_en,
input wire [7 + NUM_BITS:0] rd_addr,
input wire [7 + NUM_BITS:0] wr_addr,
input wire [15:0] data_in,
output reg [15:0] data_out,
output reg valid_out
);

parameter NUM_BLOCKS = 16;
localparam NUM_BITS = $clog2(NUM_BLOCKS);
parameter FILE = "data.hex";
  
reg [15:0] memory [0:256*NUM_BLOCKS - 1];
...
```

#### Converting from bash to makefiles
Converted my two commands.sh files to a single Makefile for the integrated_memory_controller project and started adding comments to it. There was a lot of duplication between both scripts, so the makefile helps clean that up. It also streamlines the process a bit, by making sure we don't rerun synthesis or pnr if the verilog files have not been changed. I only did this for this subdirectory, as it is really only useful if there are multiple bash scripts, but I can do it for the other sub directories if desired. 
## Next
- Combine the memory controller with a different circuit using warmbooting
	- Finish commenting makefile
	- Integrate mapping of bram locations with the other python code
	- Test demo
- Figure out how to use the FPGA to modify the SPI flash in cases where we're not just modifying the BRAM
- Further iceprog optimizations
	- Being able to specify a list of bitstreams to upload, with a hold time for evaluation, so that we only have to init the FTDI once

[[2025-07-08|prev]] [[2025-07-10|next]]
