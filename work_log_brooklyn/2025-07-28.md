## Log
- My current plan for what is going on with bitstreams on the pico:
	1. Bitstream pipeline initialized 
		* FPGA reset, initialized, data ready on Host PC
		* Host PC sends triggers start of pipeline
	2. Bitstreams transferred over USB Data line (potential compressed) into Pico DMA (possibly into usb FIFO, I think I saw somewhere that the raw FIFO is very fast)
		- Maybe sends into a buffer in RAM to allow evaluation and USB transfer at the same time?
	3. Pico DMA moves data into the FPGA flashing PIO at high speeds
	4. Bitstream write completes
	5. Measurement of bitstream figure of merit
	6. Evaluation of bitstream measurements while new bitstream is being loaded

- Predicted chokepoints of above strategy:
	- USB transfer times will likely be slow, on the order of 100s of ms I think
		- Theoretically our usecase of like large raw, unformatted binary should be quite fast.
		- However benchmarks vary kinda wildly, anywhere from like [80kB/s](https://forums.raspberrypi.com/viewtopic.php?t=332254) to [450kB/s](https://github.com/carterturn/pico_fast_serial) 
	- FPGA flashing also on the order of 100ms 
		- currently consistently about 130ms using micropython 
		- Likely faster with just reading a raw array through DMA->PIO->FPGA instead of whatever happens with python open("file.bin")
- For populations below some number (~30ish without compression) the entire population can be flashed on at setup time and then just cycled through with bitstream management on the Pico
	- Higher possible with compression
	- This seems like it would be kinda divorced from our current python refactor? like a lot of logic would have to be hardcoded in C/C++ for the pico, is this the intended usecase for Bitstream our bitstream evolution tool?
	- 
- **mip** packages must be **installed using mpremote**
## Next
- [ ]

[[2025-07-25|prev]] [[2025-07-29|next]]
